Exercise 2: MNIST
In this exercise, we consider the famous MNIST dataset, which is loaded below. The part of the dataset loaded as testX and testY will be reserved for testing - i.e. these cannot be used at all during training.

It might be a good idea to only use part of the dataset (X and Y) while tuning parameters (in order to reduce the training-time).

testX, test
%matplotlib inline
import tflearn.datasets.mnist as mnist
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
?
?
X, y, testX, testY = mnist.load_data()
Extracting mnist/train-images-idx3-ubyte.gz
Extracting mnist/train-labels-idx1-ubyte.gz
Extracting mnist/t10k-images-idx3-ubyte.gz
Extracting mnist/t10k-labels-idx1-ubyte.gz
The code-snippet below can be used to see the digits corresponding to individual digits:

import matplotlib.pyplot as plt
index = 1
?
plt.imshow(X[index].reshape(28,28),cmap=plt.cm.gray_r)
plt.show()

a) Split the training data into a training and a validation set.

X_train, X_test, Y_train, Y_test = train_test_split(X, y)
?
#It would make sense to do b) before a), since we have redundant steps in splitting the reducedDigits variabel. We could make the reduced digits, and then split the dataset with 2 digits, into test and train.
Binary classification
b) To begin with, in order to make things a little bit simpler (and faster!), extract from the data a binary subset, that only contains the data for two selected digits:

import numpy as np
?
reducedDigits_train = np.isin(Y_train, [1,8])
reducedDigits_test = np.isin(Y_test, [1,8])
?
X_train, Y_train = X_train[reducedDigits_train], Y_train[reducedDigits_train]
X_test, Y_test = X_test[reducedDigits_test], Y_test[reducedDigits_test]
?
display(Y_train,X_train)
?
#As we display, we see that the corresponding 1 and 8 Y values, has a 728 long element in the array, with values between 0 and 255 for each pixel
array([8, 8, 8, ..., 1, 1, 1], dtype=uint8)
array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
c) Learn different SVM models by varying the kernel function. For each configuration, determine the time it takes to learn the model, and the accuracy on the validation data. Caution: for some configurations, learning here can take a little while (several minutes).

or ax, C in zip(axes, [0.1, 1, 1000]):
    for a, kernel in zip(ax,['poly','rbf','sigmoid','precomputed']):
        start = time.time()
        svm = SVC(C=C, gamma=1, kernel=kernel).fit(X_train,Y_train)
        end = time.time()
        #mglearn.discrete_scatter(X_train[:,0],X_train[:,1], Y_train, ax=a)
        #mglearn.plots.plot_2d_separator(svm, X_train_subset, eps=.5, fill=True, alpha=0.3, ax=a)
        #a.set_title("C={:.3f}, gamma={:.3f}".format(C,gamma))
        print("C: " + str(C) + " Kernel: "+ kernel + " Time: " + str(end - start) + "Seconds")
        print("Accuracy on training subset: {:.2f}".format(svm.score(X_train, Y_train)))
        print("Accuracy on test subset: {:.2f}".format(svm.score(X_test,Y_test)))
        print("")
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
import time
import mglearn
?
fig, axes = plt.subplots(3,3,figsize=(10,3))
?
for ax, C in zip(axes, [0.1, 1, 1000]):
    for a, kernel in zip(ax,['poly','rbf','sigmoid','precomputed']):
        start = time.time()
        svm = SVC(C=C, gamma=1, kernel=kernel).fit(X_train,Y_train)
        end = time.time()
        #mglearn.discrete_scatter(X_train[:,0],X_train[:,1], Y_train, ax=a)
        #mglearn.plots.plot_2d_separator(svm, X_train_subset, eps=.5, fill=True, alpha=0.3, ax=a)
        #a.set_title("C={:.3f}, gamma={:.3f}".format(C,gamma))
        print("C: " + str(C) + " Kernel: "+ kernel + " Time: " + str(end - start) + "Seconds")
        print("Accuracy on training subset: {:.2f}".format(svm.score(X_train, Y_train)))
        print("Accuracy on test subset: {:.2f}".format(svm.score(X_test,Y_test)))
        print("")
C: 0.1 Kernel: poly Time: 3.9521725177764893Seconds
Accuracy on training subset: 1.00
Accuracy on test subset: 0.99

C: 0.1 Kernel: rbf Time: 82.68972659111023Seconds
Accuracy on training subset: 0.53
Accuracy on test subset: 0.53

C: 0.1 Kernel: sigmoid Time: 86.90286874771118Seconds
Accuracy on training subset: 0.53
Accuracy on test subset: 0.53

C: 1 Kernel: poly Time: 3.412508010864258Seconds
Accuracy on training subset: 1.00
Accuracy on test subset: 0.99

C: 1 Kernel: rbf Time: 215.6913561820984Seconds
Accuracy on training subset: 1.00
Accuracy on test subset: 0.81

C: 1 Kernel: sigmoid Time: 77.12863826751709Seconds
Accuracy on training subset: 0.53
Accuracy on test subset: 0.53

C: 1000 Kernel: poly Time: 3.5632987022399902Seconds
Accuracy on training subset: 1.00
Accuracy on test subset: 0.99

C: 1000 Kernel: rbf Time: 218.5870509147644Seconds
Accuracy on training subset: 1.00
Accuracy on test subset: 0.82

C: 1000 Kernel: sigmoid Time: 78.35971522331238Seconds
Accuracy on training subset: 0.30
Accuracy on test subset: 0.31


start = time.time()
svm = SVC(kernel='rbf',C=10,gamma=1).fit(X_train,Y_train)
end = time.time()
print("Accuracy obtained with a general SVM: {:.2f}".format(svm.score(X_train,Y_train)))
?
print("Execution time for the script:", end-start, "seconds")
Accuracy obtained with a general SVM: 1.00
Execution time for the script: 201.47434043884277 seconds
start = time.time()
svm = SVC(kernel='sigmoid',C=10,gamma=1).fit(X_train,Y_train)
end = time.time()
print("Accuracy obtained with a general SVM: {:.2f}".format(svm.score(X_train,Y_train)))
?
print("Execution time for the script:", end-start, "seconds")
Accuracy obtained with a general SVM: 0.48
Execution time for the script: 79.25784993171692 seconds
start = time.time()
svm = SVC(kernel='sigmoid',C=1,gamma=1).fit(X_train,Y_train)
end = time.time()
print("Accuracy obtained with a general SVM: {:.2f}".format(svm.score(X_train,Y_train)))

start = time.time()
svm = SVC(kernel='sigmoid',C=1,gamma=1).fit(X_train,Y_train)
end = time.time()
print("Accuracy obtained with a general SVM: {:.2f}".format(svm.score(X_train,Y_train)))
?
print("Execution time for the script:", end-start, "seconds")
Accuracy obtained with a general SVM: 0.54
Execution time for the script: 72.85031628608704 seconds
d) Find a way to extract the misclassified test cases. Inspect some misclassified cases and display them along with their correct label. Do they correspond to hard to recognize digits (also for the human reader)?

misclassified = np.where(Y_test != svm.predict(X_test))
misclassified = misclassified[0]
predictions = svm.predict(X_test)
for i in range(8):
    print("Prediction: "  + str(predictions[misclassified[i]]))
    print("Actual: " + str(Y_test[misclassified[i]]))
    
    plt.imshow(X_test[misclassified[i]].reshape(28,28),cmap=plt.cm.gray_r)
    plt.show()
svm = SVC(C=1, gamma=1, kernel='poly').fit(X_train,Y_train)
print("Accuracy on training subset: {:.2f}".format(svm.score(X_train, Y_train)))
print("Accuracy on test subset: {:.2f}".format(svm.score(X_test, Y_test)))
import matplotlib.pyplot as plt
?
?
misclassified = np.where(Y_test != svm.predict(X_test))
misclassified = misclassified[0]
predictions = svm.predict(X_test)
for i in range(8):
    print("Prediction: "  + str(predictions[misclassified[i]]))
    print("Actual: " + str(Y_test[misclassified[i]]))
    
    plt.imshow(X_test[misclassified[i]].reshape(28,28),cmap=plt.cm.gray_r)
    plt.show()
Accuracy on training subset: 1.00
Accuracy on test subset: 0.99
Prediction: 1
Actual: 8

Prediction: 1
Actual: 8

Prediction: 8
Actual: 1

Prediction: 1
Actual: 8

Prediction: 1
Actual: 8

Prediction: 8
Actual: 1

Prediction: 1
Actual: 8

Prediction: 1
Actual: 8

e) How do results (time and accuracy) change, depending on whether you consider an 'easy' binary task (e.g. distinguishing '1' and '0') or a more difficult one (e.g. '4' vs. '5'). This exercise requires you to make new datasets with different values for 'digit1' and 'digit2'.

X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X, y)
?
reducedDigits_train2 = np.isin(Y_train2, [1,7])
reducedDigits_test2 = np.isin(Y_test2, [1,7])
?
X_train2, Y_train2 = X_train2[reducedDigits_train2], Y_train2[reducedDigits_train2]
X_test2, Y_test2 = X_test2[reducedDigits_test2], Y_test2[reducedDigits_test2]
?
?
for ax, C in zip(axes, [1]):
    for a, kernel in zip(ax,['poly','rbf']):
        start = time.time()
        svm = SVC(C=C, gamma=0.1, kernel=kernel).fit(X_train2,Y_train2)
        end = time.time()
        #mglearn.discrete_scatter(X_train[:,0],X_train[:,1], Y_train, ax=a)
        #mglearn.plots.plot_2d_separator(svm, X_train_subset, eps=.5, fill=True, alpha=0.3, ax=a)
        #a.set_title("C={:.3f}, gamma={:.3f}".format(C,gamma))
        print("C: " + str(C) + " Kernel: "+ kernel + " Time: " + str(end - start) + "Seconds")
        print("Accuracy on training subset: {:.2f}".format(svm.score(X_train2, Y_train2)))
        print("Accuracy on test subset: {:.2f}".format(svm.score(X_test2,Y_test2)))
        print("")
        
        
        
        
misclassified2 = np.where(Y_test2 != svm.predict(X_test2))
misclassified2 = misclassified2[0]
predictions = svm.predict(X_test2)
for i in range(8):
    print("Prediction: "  + str(predictions[misclassified2[i]]))
    print("Actual: " + str(Y_test2[misclassified2[i]]))
    
    plt.imshow(X_test2[misclassified2[i]].reshape(28,28),cmap=plt.cm.gray_r)
    plt.show()
?
C: 1 Kernel: poly Time: 2.669475555419922Seconds
Accuracy on training subset: 1.00
Accuracy on test subset: 1.00

C: 1 Kernel: rbf Time: 27.217586040496826Seconds
Accuracy on training subset: 1.00
Accuracy on test subset: 0.99

Prediction: 1
Actual: 7

Prediction: 7
Actual: 1

Prediction: 7
Actual: 1

Prediction: 7
Actual: 1

Prediction: 7
Actual: 1

Prediction: 7
Actual: 1

Prediction: 7
Actual: 1

Prediction: 7
Actual: 1

Multiclass classification
f) [Discussion only] Explain how a binary classifier, such as an SVM, can be applied to a multiclass classification problem, such as recognizing all 10 digits in the MNIST dataset.

There is a binary classifier for each class, in the model (in this case 10 different).
Each prediciton will consist of a binary decision between these 10 different classes, meaning that there either it is 
?
Type Markdown and LaTeX: ??2
g) From the binary classification exercise above, identify a good configuration that gives a reasonable combination of accuracy and runtime. Use this configuration to perform a full classification of the 10 classes in the original dataset. Report the accuracy obtained on the test data.

model of the data in black/white, is granted good towards the mnist dataset

#The rbf kernel radial basis function, looks at the actual values of each pixel
#the poly kernel, will make a polonomial of the pixel mapping, and look at combinations.
#As we see in the above exercises, we identify poly kernel as being fastest and most reliable.
#This makes sence, since mapping a polonomial model of the data in black/white, is granted good towards the mnist dataset
?
start = time.time()
svm = SVC(kernel='poly',C=1,gamma=1).fit(X,y)
end = time.time()
print("Accuracy obtained with a general SVM: {:.2f}".format(svm.score(X,y)))
?