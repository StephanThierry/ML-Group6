{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Machine Learning</h1><h2 align=\"center\" style=\"margin:10px\">Assignment 3</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Group 6:\n",
    "    287040 Stephan Thierry\n",
    "    254172 Kasper Holst Daugaard \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignments below should be solved and documented as a mini-project that will form the basis for the\n",
    "examination. When solving the exercises it is important that you\n",
    "\n",
    "  * document all relevant results and analyses that you have obtained/performed during the exercises\n",
    "  * try to relate your results to the theoretical background of the methods being applied.\n",
    "\n",
    "Feel free to add cells if you need to. The easiest way to convert to pdf is to save this notebook as .html (File-->Download as-->HTML) and then convert this html file to pdf. You can also export as pdf directly, but here you need to watch your margins as the converter will cut off your code (i.e. make vertical code!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Linear vs nonlinear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show a dataset that cannot be linearly separated. In this exercise, we will use the default parameters for all classifiers (except the custom SVM in exercise d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X, y = make_circles(1000, factor=0.0, noise=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Plot the dataset e.g. using the `discrete_scatter`-function from mglearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:,0],X[:,1], y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b) Split the dataset into train and test-sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we could split using a random_state that demonstrates the outcome that we want to show (3). Other random_state's (4) will show a much lower accuracy - but knowing that data it should be about 50/50.\n",
    "\n",
    "Instead we stratify on y so the data is split evenly and there is not an overrepresentation of one classification in either of the new datasets\n",
    "\n",
    "We don't set \"test_size\" so we use the default split of 75/25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "c) Train a logistic regression on the dataset, and compute the classification accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Logreg = LogisticRegression(solver='lbfgs')\n",
    "##Logreg.fit(X_train,y_train)\n",
    "Logreg.fit(X_train,y_train)\n",
    "print (Logreg.predict_proba([X_test[0]]))\n",
    "print (Logreg.predict_proba([X_test[10]]))\n",
    "print (Logreg.predict_proba([X_test[100]]))\n",
    "\n",
    "print(\"Accuracy on Training set is: {:.2f}\".format(Logreg.score(X_train,y_train)))\n",
    "print(\"Accuracy on Test set is: {:.2f}\".format(Logreg.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model on the training set at test the accuracy on the test set. As expected it's around 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Plot the decision boundary for the logistic regression (e.g. using the `plot_2d_separator`-function from mglearn), and use this to investigate why the algorithm does not give a good result in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logreg.fit(X_train,y_train)\n",
    "from mglearn.plots import plot_2d_separator\n",
    "mglearn.discrete_scatter(X_train[:,0],X_train[:,1], y_train)\n",
    "plt.plot(X_train[0,0],X_train[0,1],'k.')\n",
    "plt.legend()\n",
    "plot_2d_separator(Logreg, X_train, fill=True, eps=0.4, alpha=.7)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the linier boundry in a 2d space will not accurately seperate the data. It's split down the middle so ~50/50 is the best we can hope for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "e) Think of a feature you could add to this dataset to make it linearly separable. \n",
    "Add this feature, retrain the logistic regression classifier, and compute the accuracy again. Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is a circle inside another circle, adding \"distance to the center\" as a feature would be useful to the liner (plane) seperation \n",
    "\n",
    "From using distance from scipy.spatial we calculate the euclidean distance to the center. The center is at 0,0 - but if it was not we could calculate is and change the center-variable accordingly\n",
    "\n",
    "See code comments for details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add distance as new feature\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "center = [0,0];\n",
    "X2 = np.empty([0,3])\n",
    "\n",
    "#We run through each row of X and add the distance as a feature in each row\n",
    "for eachitem in X:\n",
    "    # Calculate distance to center\n",
    "    dist = distance.euclidean(center, eachitem)\n",
    "    # Add (insert) the distance to the current row and add (vertical-stack) the result to the resultset: X2 \n",
    "    X2 = np.vstack((X2,np.insert(eachitem, 2, dist)))\n",
    "\n",
    "## print(X2)\n",
    "\n",
    "# We do a new split so test and traing sets contain the new feature\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, stratify=y)\n",
    "\n",
    "## We use the X_plot and y_plot vaiables so we can run the plot-code on both \n",
    "##  the full dataset and the training-set by only changing 2 lines\n",
    "\n",
    "##X_plot = X2\n",
    "##y_plot = y\n",
    "\n",
    "X_plot = X2_train\n",
    "y_plot = y2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D, axes3d\n",
    "figure = plt.figure()\n",
    "# visualize in 3D\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "# plot first all the points with y == 0, then all with y == 1\n",
    "mask = y_plot == 0\n",
    "ax.scatter(X_plot[mask, 0], X_plot[mask, 1], X_plot[mask, 2], c='b',\n",
    "    cmap=mglearn.cm2, s=60)\n",
    "ax.scatter(X_plot[~mask, 0], X_plot[~mask, 1], X_plot[~mask, 2], c='r', marker='^',\n",
    "    cmap=mglearn.cm2, s=60)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Distance to center\")\n",
    "\n",
    "linear_3d = Logreg.fit(X_plot, y_plot)\n",
    "coef, intercept = linear_3d.coef_.ravel(), linear_3d.intercept_\n",
    "# show linear decision boundary\n",
    "figure = plt.figure()\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "xx = np.linspace(X_plot[:, 0].min() - 2, X_plot[:, 0].max() + 2, 50)\n",
    "yy = np.linspace(X_plot[:, 1].min() - 2, X_plot[:, 1].max() + 2, 50)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
    "ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n",
    "ax.scatter(X_plot[mask, 0], X_plot[mask, 1], X_plot[mask, 2], c='b',\n",
    "    cmap=mglearn.cm2, s=60)\n",
    "ax.scatter(X_plot[~mask, 0], X_plot[~mask, 1], X_plot[~mask, 2], c='r', marker='^',\n",
    "    cmap=mglearn.cm2, s=60)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Distance to center\")\n",
    "\n",
    "print('Accuracy on training set: {:.2f}'.format(linear_3d.score(X2_train, y2_train)))\n",
    "print('Accuracy on test set: {:.2f}'.format(linear_3d.score(X2_test, y2_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "f) Now, return to the original dataset (without the extra feature), and train a kernelized SVM on the dataset. Compute the accuracy and plot the decision boundary. Compare to your previous results and discuss the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(gamma='auto').fit(X_train,y_train)\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "##fig, axes = plt.subplots(figsize=(10, 3))\n",
    "\n",
    "plot_2d_separator(svm, X, fill=True, eps=0.5, alpha=.7 )\n",
    "##plt.figure(figsize=(20,20))\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svm.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(svm.score(X_test, y_test)))\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernelized SVM does not rely on a liniear seperation of the data, so we don't need to move into a higher dimintion to achive accurate seperation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(25,20))\n",
    "\n",
    "for ax, C in zip(axes, [0.1, 1, 1000]):\n",
    "    for a, gamma in zip(ax,[0.1, 1, 10]):\n",
    "        svm = SVC(C=C, gamma=gamma).fit(X,y)\n",
    "        mglearn.discrete_scatter(X[:,0],X[:,1], y, ax=a)\n",
    "        mglearn.plots.plot_2d_separator(svm, X, eps=.5, fill=True, alpha=0.3, ax=a)\n",
    "        a.set_title(\"C={:.3f}, gamma={:.3f}\".format(C,gamma))\n",
    "        \n",
    "svm.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svm.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(svm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(axes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we consider the famous MNIST dataset, which is loaded below. The part of the dataset loaded as `testX` and `testY` will be reserved for testing - i.e. these cannot be used at all during training. \n",
    "\n",
    "It might be a good idea to only use part of the dataset (`X` and `Y`) while tuning parameters (in order to reduce the training-time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn.datasets.mnist as mnist\n",
    "X, Y, testX, testY = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code-snippet below can be used to see the digits corresponding to individual digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "index = 1\n",
    "\n",
    "plt.imshow(X[index].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Split the training data into a training and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, Y, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) To begin with, in order to make things a little bit simpler (and faster!), extract from the data a binary subset, that only contains the data for two selected digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Take two digits 5 and 2\n",
    "\n",
    "reducedIndexes = np.isin(y, [2,5])\n",
    "X_subset, y_subset = X[reducedIndexes], y[reducedIndexes]\n",
    "\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_subset, y_subset, stratify=Y)\n",
    "\n",
    "print(y_train.size) # 41250 elements\n",
    "print(y_train_subset.size) # reduced to 7843 elements (if we dont use the \"break\")\n",
    "\n",
    "plt.imshow(X_train_subset[0].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Learn different SVM models by varying the kernel function. For each configuration,\n",
    "determine the time it takes to learn the model, and the accuracy on the validation data. *Caution*: for some\n",
    "configurations, learning here can take a little while (several minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It must be one of ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_subset, y_train_subset, stratify=y_train_subset)\n",
    "\n",
    "import mglearn\n",
    "import time\n",
    "numberList = [1, 2, 3]\n",
    "kernelList = [1, 2, 3, 4]\n",
    "\n",
    "for C in [0.1, 1, 1000]:\n",
    "    for gamma in [0.1, 1, 10]:\n",
    "        for kernel in ['poly','rbf','sigmoid']:\n",
    "            start = time.time()\n",
    "            svm = SVC(C=C, gamma=gamma, kernel=kernel).fit(X_train,y_train)\n",
    "            end = time.time()\n",
    "            #mglearn.discrete_scatter(X_train_subset[:,0],X_train_subset[:,1], y_train_subset, ax=a)\n",
    "            #mglearn.plots.plot_2d_separator(svm, X_train_subset, eps=.5, fill=True, alpha=0.3, ax=a)\n",
    "            #a.set_title(\"C={:.3f}, gamma={:.3f}\".format(C,1))\n",
    "            print(\"C: \" + str(C) + \" gamma: \" + str(gamma) + \" Kernel: \"+ kernel + \" Time: \" + str(end - start))\n",
    "            print(\"Accuracy on training subset: {:.2f}\".format(svm.score(X_train, y_train)))\n",
    "            print(\"Accuracy on test subset: {:.2f}\".format(svm.score(X_test, y_test)))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Find a way to extract the misclassified test cases. Inspect some misclassified cases and display them along with their correct label.\n",
    "Do they correspond to hard to recognize digits (also for the human reader)?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=1, gamma=1, kernel='poly').fit(X_train,y_train)\n",
    "print(\"Accuracy on training subset: {:.2f}\".format(svm.score(X_train, y_train)))\n",
    "print(\"Accuracy on test subset: {:.2f}\".format(svm.score(X_test, y_test)))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "misclassified = np.where(y_test != svm.predict(X_test))\n",
    "misclassified = misclassified[0]\n",
    "predictions = svm.predict(X_test)\n",
    "for i in range(8):\n",
    "    print(\"Prediction: \"  + str(predictions[misclassified[i]]))\n",
    "    print(\"Actual: \" + str(y_test[misclassified[i]]))\n",
    "    \n",
    "    plt.imshow(X_test[misclassified[i]].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) How do results (time and accuracy) change, depending on whether you consider\n",
    "an 'easy' binary task (e.g. distinguishing '1' and '0') or a more difficult one (e.g. '4' vs. '5'). This exercise\n",
    "requires you to make new datasets with different values for 'digit1' and 'digit2'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) [Discussion only] Explain how a binary classifier, such as an SVM, can be applied to a multiclass classification problem, such as recognizing all 10 digits in the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) From the binary classification exercise above, identify a good configuration that gives a reasonable combination\n",
    "of accuracy and runtime. Use this configuration to perform a full classification of the 10 classes in the\n",
    "original dataset. Report the accuracy obtained on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Regression with random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will be using the famous nycflights dataset.\n",
    "\n",
    "So far, we have only considered how to use SVMs and decision trees (and, by extension, random forests) for classification. However, both algorithms can also be used for regression tasks, as we will see in the exercises below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "a) Load the data as a pandas dataframe and display the first 5 rows of the dataset. Remove the columns `'carrier'`,`'tailnum'`,`'flight'`,`'origin'`, and `'dest'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flights= pd.read_csv('flights.csv')\n",
    "for col in ['carrier','tailnum','flight','origin','dest']:\n",
    "    flights = flights.drop(columns=col)\n",
    "print(\"Columns after drop: \" + str(flights.columns))\n",
    "\n",
    "# Display first 5 rows\n",
    "print(flights[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b) Plot the distributions for all variables (hint: use the `hist` method for the dataframe). Consider if you want to transform any of the variables, i.e. using a logarithmic transformation. Explain your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike standard regression methods, decition trees do not rely on calculating coefficients so the requirements for data preparation are not the same as for SVM or other regression-type functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "flights.hist( figsize=(14,16))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "c) Handle any nan-values in the dataset, and normalize all relevant variables. Are there any categorical variables? If so, create dummy variables for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## This section is to detect \n",
    "\n",
    "print(\"Null values exist in the dataset (true or false) \" + str(flights.isnull().any().any()))\n",
    "print()\n",
    "print(\"In what features can we find null values: \")\n",
    "print(flights.isnull().any())\n",
    "print()\n",
    "print(\"How many?\")\n",
    "print(flights.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We have the option to delete entries with Null or to replace them with another value \n",
    "#  like the mean, the mode or the median - in this case we use the \n",
    "#  'mode' meaning the most frequent observation. The code for \"mean\" is included in the comment\n",
    "\n",
    "for col in ['dep_time','dep_delay','arr_time','arr_delay','air_time','hour','minute']:\n",
    "    flights[col] = flights[col].fillna(flights[col].mode()[0])\n",
    "    #flights[col] = flights[col].fillna(flights[col].mean())\n",
    "\n",
    "# We now see that we have 0 nulls in our dataset\n",
    "print(flights.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed the obvoius categorical fields 'carrier', 'tailnum','flight','origin' and 'dest' - year and month could be candidates since they are not quanties. However, it's not clear that it will improve the model to convert them. \n",
    "\n",
    "It could be done using below code:   \n",
    "`fight_withdummies = pd.get_dummies(flights, prefix='year_', columns=['year'])`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) In the following, we are going to determine which factors cause departure time delays, and try to predict the length of these delays. However, for several departures, a *negative* delay have been reported. How do you interpret a negative delay? Consider if you want to modify the negative delays in some way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative delay is most likely a flight that arrives ahead of schedule. Two approches could be used.   \n",
    "\n",
    "1. Consider the negative delays along side the delays - because if departures during rain are sometimes delayed and sometimes are ahead of schedule then it's not clear that rain is contributing to the delay. Whereas if you filter out the negatives incorrect patterns might emerge  \n",
    "\n",
    "2. We could also choose to only look at delays and disregard flights that are ahead, since being ahead of schedule has a very limited positive impact compared to the negative of being delayed\n",
    "\n",
    "We have chosen option 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression analysis: Predicting departure time delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Extract the features and the target variable (in this case the departure time delays) from the dataframe. Split the dataset into test and train sets (technically, we ought to have done this before preprocessing. For the sake of simplicity, we do not conform to this best practice in this exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = flights['dep_delay']\n",
    "\n",
    "# We drop dep_delay from the dataset since it's now our taget - and we also drop arr_delay since that will not be known \n",
    "#  at the time we are trying to prodict departure delay.\n",
    "\n",
    "# If we leave arr_delay in as a part of the dataset, prediction will be vastly improved - but that would not be \n",
    "#  representing a real-life senario\n",
    "\n",
    "X = flights.drop(columns='dep_delay').drop(columns='arr_delay')\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Train a decision tree regressor for predicting departure time delays (you might want to experiment with a few different values of the hyperparameters to avoid too much overfitting). Plot the tree, and explain how decision trees can be used for regression analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# max_depth=9 is the tipping poing of where we only get an increased accuracy on \n",
    "#   the trainingset (overfitting) by increasing the value. If we lower the value we get a lower\n",
    "#   score on the test set (underfitting)\n",
    "tree = DecisionTreeRegressor(max_depth=9)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training data: {}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on testing data: {}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz as graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "dot_data = export_graphviz(tree, out_file=None,filled=True, rounded=True,special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Do a regression analysis as the one above, but using a random forest instead of a single decision tree. Use a grid-search to determine a good set of hyperparameters. When you have found the best model, score your model on the test set. Comment on the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# max_depth=9 - since it seemed to produce good results in the prev. example\n",
    "# max_features=5 - we want to find feature_importances_ so we reduce to a limited subset of our features\n",
    "\n",
    "RandForReg = RandomForestRegressor(max_depth=9, random_state=0, n_estimators=60, max_features=5)\n",
    "RandForReg.fit(X_train, y_train)\n",
    "\n",
    "#RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
    "#           max_features='auto', max_leaf_nodes=None,\n",
    "#           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#           min_samples_leaf=1, min_samples_split=2,\n",
    "#           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#           oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "print(RandForReg.score(X_test, y_test))\n",
    "# Acc: ~0.29, random forest is doing only slightly better (~0.03) than the standard DecisionTreeRegressor\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RandForReg.feature_importances_)\n",
    "\n",
    "n_features = X_train.columns\n",
    "plt.barh(range(len(n_features)), RandForReg.feature_importances_, align='center')\n",
    "plt.yticks(np.arange(n_features), n_features)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Plot the feature importances determined by the tree. Which feature is the most important? Do you have any idea as to why? Remove any features which cannot be used to predict departure time delays in any meaningful way, and redo the analysis. Comment on your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression analysis: Predicting arrival time delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last part of the exercise, we are going to try to predict arrival time delays as a function of departure time delays - it might be of interest to know how large a delay one should expect after the plane has departed from the airport. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Train a decision tree or random forest regressor and an OLS (Ordinary least squares) to the dataset, and see how well arrival time delay. can be predicted based on departure time delay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "flights = pd.read_csv('flights.csv')\n",
    "\n",
    "# Set all null values to the mode - except in the target 'arr_delay'\n",
    "for col in ['dep_time','dep_delay','arr_time','air_time','hour','minute']:\n",
    "    flights[col] = flights[col].fillna(flights[col].mode()[0])    \n",
    "\n",
    "# Only the target column now contains Null - drop all rows that don't have a valid target    \n",
    "flights.dropna()\n",
    "\n",
    "# The training target y is now \"arr_delay\" \n",
    "y = flights['arr_delay']\n",
    "\n",
    "# We drop the taget \"arr_delay\" from the dataset and \"air_time\" since that is not known at the \n",
    "#  time we try to make the prediction\n",
    "flights = flights.drop(columns='arr_delay').drop(columns='air_time') \n",
    "\n",
    "# We make dummy colums for the category features and drop the original column afterwards\n",
    "for col in ['carrier','tailnum','flight','origin','dest']:\n",
    "    flights = pd.get_dummies(flights, prefix=col, columns=[col])\n",
    "    #flights = flights.drop(columns=col)\n",
    "\n",
    "\n",
    "# We split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(flights, y)\n",
    "# Laptop   - 2min30\n",
    "# i9-9900k - 26 sec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flights.isnull().sum())\n",
    "#print(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RandForReg = RandomForestRegressor(max_depth=9, random_state=0, n_estimators=60, max_features=5)\n",
    "RandForReg.fit(X_train, y_train)\n",
    "\n",
    "print(RandForReg.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) Plot the arrival time delays as a function of the departure time delay, and show the predictions from each of the two regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k) Based on the results obtained above, make a plot that extrapolates a little bit in order to predict delays slightly larger than the largest delay found in the dataset. Which model do you think gives the most trustworthy extrapolation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l) Hopefully you found that it is possible to predict arrival time delays quite confidently from departure time delays. See if you can improve these predictions by including some (or all) of the other features. You are encouraged to try out several different machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
